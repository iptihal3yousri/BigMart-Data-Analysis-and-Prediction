# -*- coding: utf-8 -*-
"""TECHNOCOLABS mini_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DuQiuUBAWtF-lL5H87IDz7AeEnPlyaIw

# **Big Mart Sales Prediction**

**Problem Statement**

The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

why predicting is useful hare?
> 1. stores which play a key role in increasing sales.
2. understand the properties of products.
3. precieve challanges early.
4. aid future marketing plans.
5. predict the sales revenue.

its a regression based ML project.

**Hypothesis Generation**

> Product Visbility: Products with large display area should have higher Sales.
> Outlet_Size: Stores with large size should have higher Sales.
> Item_Type:
> 1. Products that are made from a well-known brand will have higher Sales.
2. mostly used Products will have higher Sales.

> Item_MRP: Products with least cost sould have higher Sales
> Outlet_Establishment_Year: oldest stores will have higher Sales
> Outlet_Location_city:
> 1. Store located in popular market place shoud have higher Sales.
2. City with high population should have higher Sales.

> Products with offers will have higher Sales.

**Loading Packages and Data**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from xgboost import XGBRegressor
from sklearn.svm import SVR

df_train = pd.read_csv('Train.csv')
df_test = pd.read_csv('Test.csv')

df_train.head()

df_test.head()

"""**Data Structure and Content**

The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.


Variable | Description
----------|--------------
Item_Identifier | Unique product ID
Item_Weight | Weight of product
Item_Fat_Content | Whether the product is low fat or not
Item_Visibility | The % of total display area of all products in a    store allocated to the particular product
Item_Type | The category to which the product belongs
Item_MRP | Maximum Retail Price (list price) of the product
Outlet_Identifier | Unique store ID
Outlet_Establishment_Year | The year in which store was established
Outlet_Size | The size of the store in terms of ground area covered
Outlet_Location_Type | The type of city in which the store is located
Outlet_Type | Whether the outlet is just a grocery store or some sort of supermarket
Item_Outlet_Sales | Sales of the product in the particulat store. This is the outcome variable to be predicted.

**Exploratory Data Analysis**

> check duplication
"""

df_train.duplicated().any()

df_test.duplicated().any()

print(df_train.shape)
print(df_test.shape)

df_train.info()

df_test.info()

df_train.describe()

df_test.describe()

"""> check null values"""

df_train.isnull().sum()

df_test.isnull().sum()

df_train.isnull().mean()*100

df_test.isnull().mean()*100

df_train.corr()

sns.heatmap(df_train.corr(), annot=True)

df_train['Item_Fat_Content'].value_counts()

"""in the previous cell 'Item_Fat_Content' has two values mean the same.

        Low Fat=LF=low fat
        Regular=reg

so we need to solve this issue, and will be solved in the feature engineering section
"""

df_train['Outlet_Size'].value_counts()

df_train['Item_Type'].unique()

df_train['Outlet_Location_Type'].value_counts()

df_train['Outlet_Type'].value_counts()

"""according to the previous cells there is no issues in the other features except  'Item_Fat_Content'

check outliers
"""

sns.boxplot(df_train['Item_Weight'])

sns.boxplot(df_train['Item_MRP'])

sns.boxplot(df_train['Outlet_Establishment_Year'])

sns.boxplot(df_train['Item_Visibility'])

sns.boxplot(df_train['Item_Outlet_Sales'])

"""> see the outlier Values"""

max_Item_Outlet_Sales = df_train['Item_Outlet_Sales'].quantile(.97)
new_df = df_train[(df_train['Item_Outlet_Sales'] < max_Item_Outlet_Sales) ]
sns.boxplot(new_df['Item_Outlet_Sales'])

max_Item_Visibility = df_train['Item_Visibility'].quantile(.98)
new_df = df_train[(df_train['Item_Visibility'] < max_Item_Visibility) ]
sns.boxplot(new_df['Item_Visibility'])

"""we will treat outliears in the feature engineering section

**Univariate Analysis**

> distplot for numerical data
"""

sns.distplot(df_train['Item_Weight'])

sns.distplot(df_train['Item_MRP'])

sns.distplot(df_train['Item_Visibility'])

sns.distplot(df_train['Item_Outlet_Sales'])

"""the previous two charts are right skewed distribution.
will be solved in the feature engineering section

> countplots for categorial data
"""

plt.figure(figsize=(7,5))
sns.countplot(data=df_train, x='Item_Fat_Content')
plt.show()

#   Item_Type " Category"
plt.figure(figsize=(24,6))
sns.countplot(data=df_train, x='Item_Type')
plt.show()

plt.figure(figsize=(24,6))
sns.countplot(data=df_train, x='Outlet_Identifier')
plt.show()

#   Store Type
plt.figure(figsize=(7,5))
sns.countplot(data=df_train, x='Outlet_Type')
plt.show()

#   Store Location Type
plt.figure(figsize=(7,5))
sns.countplot(data=df_train, x='Outlet_Location_Type')
plt.show()

# store_size
plt.figure(figsize=(7,5))
sns.countplot(data=df_train, x='Outlet_Size')
plt.show()

"""> histogram for numerical"""

plt.hist(x=df_train['Outlet_Establishment_Year'])
plt.show()

"""from the previous histogram,i think its better to work with the age of the store instead its Establishment Year"""

df_train['Outlet_Establishment_Year'] = 2013 - df_train['Outlet_Establishment_Year']
df_test['Outlet_Establishment_Year'] = 2013 - df_test['Outlet_Establishment_Year']

plt.hist(x=df_train['Outlet_Establishment_Year'])
plt.show()

"""box plots done before in the EDA section to check outliers

**Bivariate Analysis**

> For Categorical
"""

# BarPlot to visualize Sales depends on Item_Type
plt.figure(figsize=(24,6))
sns.barplot(data=df_train, x='Item_Type', y='Item_Outlet_Sales')
plt.show()

# BarPlot to visualize Sales depends on store Identifier
plt.figure(figsize=(10,4))
sns.barplot(data=df_train, x='Outlet_Identifier', y='Item_Outlet_Sales')
plt.show()

# BarPlot to visualize Sales depends on store size
plt.figure(figsize=(7,5))
sns.barplot(data=df_train, x='Outlet_Size', y='Item_Outlet_Sales')
plt.show()

# BarPlot to visualize Sales per depends on Location Type
plt.figure(figsize=(7,5))
sns.barplot(data=df_train, x='Outlet_Location_Type', y='Item_Outlet_Sales')
plt.show()

# BarPlot to visualize Sales depends on store Type
plt.figure(figsize=(7,5))
sns.barplot(data=df_train, x='Outlet_Type', y='Item_Outlet_Sales')
plt.show()

"""> For Numeric"""

# ScatterPlot to visualize Sales depends on Item MRP
plt.figure(figsize=(10,7))
plt.scatter(df_train['Item_MRP'], df_train['Item_Outlet_Sales'])
plt.show()

# ScatterPlot to visualize Sales depends on Item_Visibilty
plt.figure(figsize=(10,7))
plt.scatter(df_train['Item_Visibility'], df_train['Item_Outlet_Sales'])
plt.show()

# ScatterPlot to visualize Sales depends on Item_Weight
plt.figure(figsize=(10,7))
plt.scatter(df_train['Item_Weight'], df_train['Item_Outlet_Sales'])
plt.show()

"""correlation matrix done before in the EDA section

**Multivariate plots**
"""

# store Type in all store location based on sales.
plt.figure(figsize=(10,5))
sns.barplot(data=df_train, x='Outlet_Location_Type', y='Item_Outlet_Sales', hue='Outlet_Type')
plt.show()

# store Location Type of store Type based on sales.
plt.figure(figsize=(10,5))
sns.barplot( data=df_train, x='Outlet_Location_Type', y='Item_Outlet_Sales', hue='Outlet_Type')

# store Type of store Location Type based on sales.
plt.figure(figsize=(10,5))
sns.barplot(data=df_train, x='Outlet_Type', y='Item_Outlet_Sales', hue='Outlet_Location_Type')
plt.legend()

"""**Missing Value Treatment**"""

df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean(), inplace = True)
df_test['Item_Weight'].fillna(df_test['Item_Weight'].mean(), inplace = True)

df_train['Outlet_Size'].value_counts()

df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0], inplace = True)
df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0], inplace = True)

df_train.isnull().sum()

df_test.isnull().sum()

"""**Feature Engineering**

Feature Preprocessing

1. feature transformation

> > treat the right skewed distribution

its optional to do this step.
when i tried to do feature transformation this improves the accuracy of each model, but changes the nature of the data, so i skipped this step
"""

# df_train['Item_Outlet_Sales'] = np.sqrt(df_train['Item_Outlet_Sales'])
# sns.distplot(df_train['Item_Outlet_Sales'])

# df_train['Item_Visibility'] = np.sqrt(df_train['Item_Visibility'])
# df_test['Item_Visibility'] = np.sqrt(df_test['Item_Visibility'])
# sns.distplot(df_train['Item_Visibility'])
# sns.distplot(df_test['Item_Visibility'])

"""2. Feature mismatch"""

df_train['Item_Fat_Content'].value_counts()

"""in the previous cell  'Item_Fat_Content' has two values mean the same.
>  
1. Low Fat=LF=low fat
2. Regular=reg

so we need to solve this issue
"""

mapping = {"Low Fat": "Low Fat", "LF": "Low Fat", "low fat": "Low Fat", "Regular": "Regular", "reg": "Regular"}
df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].map(mapping)
df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].map(mapping)

df_train['Item_Fat_Content'].value_counts()

"""looking to 'Item_Identifier' column you can see too much unique values but they all have common part, so we will work on this common part"""

#     FD : Food     ,     DR : Drinks     ,     NC : Non-Consumable
df_train['Item_Identifier'] = df_train['Item_Identifier'].apply(lambda x: x[:2])
df_test['Item_Identifier'] = df_test['Item_Identifier'].apply(lambda x: x[:2])

"""3. Feature Scaling"""

# MinMaxScaler
scaler = MinMaxScaler()
df_train['Item_MRP'] = scaler.fit_transform(df_train['Item_MRP'].to_numpy().reshape(-1, 1))
df_test['Item_MRP'] = scaler.fit_transform(df_test['Item_MRP'].to_numpy().reshape(-1, 1))

"""Dimesnsionality reduction"""

df_train.drop(['Outlet_Identifier'],axis=1,inplace=True)
df_test.drop(['Outlet_Identifier'],axis=1,inplace=True)

"""Remove Outliers

> before in the EDA section i checked the outlier values start and Know the values needs to be droped
"""

df_train.drop(df_train[df_train['Item_Outlet_Sales'] > 6094.307].index, inplace = True)

sns.boxplot(df_train['Item_Outlet_Sales'])

df_train.drop(df_train[df_train['Item_Visibility'] >0.187].index, inplace = True)

sns.boxplot(df_train['Item_Visibility'])

df_test.drop(df_test[df_test['Item_Visibility'] >0.185].index, inplace = True)

sns.boxplot(df_test['Item_Visibility'])

"""**Encoding Categorical Variables**"""

df_train['Item_Fat_Content'].unique()

mapping = {"Low Fat": 0, "Regular": 1}
df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].map(mapping)
df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].map(mapping)

df_train['Item_Type'].unique()

df_test['Item_Type'].unique()

mapping = {"Dairy":0, "Soft Drinks":1, "Meat":2, "Fruits and Vegetables":3, "Household":4,
           "Baking Goods":5, "Snack Foods":6, "Frozen Foods":7, "Breakfast":8, "Health and Hygiene":9,
           "Hard Drinks":10, "Canned":11, "Breads":12, "Starchy Foods":13, "Others":14, "Seafood":15}
df_train['Item_Type'] = df_train['Item_Type'].map(mapping)
df_test['Item_Type'] = df_test['Item_Type'].map(mapping)

df_train['Outlet_Size'].unique()

mapping = {"Medium": 0, "High": 1, "Small": 2}
df_train['Outlet_Size'] = df_train['Outlet_Size'].map(mapping)
df_test['Outlet_Size'] = df_test['Outlet_Size'].map(mapping)

df_train['Outlet_Location_Type'].unique()

mapping = {"Tier 1": 0, "Tier 2": 1, "Tier 3": 2}
df_train['Outlet_Location_Type'] = df_train['Outlet_Location_Type'].map(mapping)
df_test['Outlet_Location_Type'] = df_test['Outlet_Location_Type'].map(mapping)

df_train['Outlet_Type'].unique()

mapping = {"Grocery Store": 0, "Supermarket Type1": 1, "Supermarket Type2": 2, "Supermarket Type3": 3}
df_train['Outlet_Type'] = df_train['Outlet_Type'].map(mapping)
df_test['Outlet_Type'] = df_test['Outlet_Type'].map(mapping)

df_train['Item_Identifier'].unique()

mapping = {"FD": 0, "DR": 1, "NC": 2}
df_train['Item_Identifier'] = df_train['Item_Identifier'].map(mapping)
df_test['Item_Identifier'] = df_test['Item_Identifier'].map(mapping)

"""**Preprocessing Data**"""

df_train.head()

df_test.head()

y = df_train.Item_Outlet_Sales
df_train.drop(columns = ['Item_Outlet_Sales'], inplace = True)
x = df_train

"""**Modeling**

> Linear regression is probably one of the most important and widely used regression techniques. It’s among the simplest regression methods. One of its main advantages is the ease of interpreting results.
Problem Formulation

> When implementing linear regression of some dependent variable 𝑦 on the set of independent variables 𝐱 = (𝑥₁, …, 𝑥ᵣ), where 𝑟 is the number of predictors, you assume a linear relationship between 𝑦 and 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. This equation is the regression equation. 𝛽₀, 𝛽₁, …, 𝛽ᵣ are the regression coefficients, and 𝜀 is the random error.

> Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with 𝑏₀, 𝑏₁, …, 𝑏ᵣ. These estimators define the estimated regression function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ. This function should capture the dependencies between the inputs and output sufficiently well.

> The estimated or predicted response, 𝑓(𝐱ᵢ), for each observation 𝑖 = 1, …, 𝑛, should be as close as possible to the corresponding actual response 𝑦ᵢ. The differences 𝑦ᵢ - 𝑓(𝐱ᵢ) for all observations 𝑖 = 1, …, 𝑛, are called the residuals. Regression is about determining the best predicted weights—that is, the weights corresponding to the smallest residuals.

> To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations 𝑖 = 1, …, 𝑛: SSR = Σᵢ(𝑦ᵢ - 𝑓(𝐱ᵢ))². This approach is called the method of ordinary least squares.

> Random Forest is a technique that uses ensemble learning, that combines many weak classifiers to provide solutions to complex problems.

> As the name suggests random forest consists of many decision trees. Rather than depending on one tree it takes the prediction from each tree and based on the majority votes of predictions, predicts the final output. Don’t worry if you haven’t read about decision trees, I have that part covered in this article.



> Ensemble Techniques

> Suppose you want to purchase a house, will you just walk into society and purchase the very first house you see, or based on the advice of your broker will you buy a house? It’s highly unlikely.

> You would likely browse a few web portals, checking for the area, number of bedrooms, facilities, price, etc. You will also probably ask your friends and colleagues for their opinion. In short, you wouldn’t directly reach a conclusion, but will instead make a decision considering the opinions of other people as well.

> Ensemble techniques work in a similar manner, it simply combines multiple models. Thus, a collection of models is used to make predictions rather than an individual model and this will increase the overall performance. Let’s understand 2 main ensemble methods in Machine Learning:

> 1. Bagging – Suppose we have a dataset, and we make different models on the same dataset and combine it, will it be useful? No right? There is a high chance we’ll get the same results since we are giving the same input. So instead we use a technique called bootstrapping. In this, we create subsets of the original dataset with replacement. The size of the subsets is the same as the size of the original set. Since we do this with replacement so there is a high chance that we provide different data points to our models.

> 2. Boosting – Suppose any data point in your observation has been incorrectly classified by your 1st model, and then the next (probably all the models), will combine the predictions provide better results? Off-course it’s a big NO.

> Boosting technique is a sequential process, where each model tries to correct the errors of the previous model. The succeeding models are dependent on the previous model.

> It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example, ADA BOOST, XG BOOST.

split the data
"""

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state = 1)

"""**RandomForestRegressor**"""

randomForecetModel = RandomForestRegressor(n_estimators=200, criterion='poisson', max_depth=10, max_features=6, oob_score=True).fit(x_train, y_train)
pre = randomForecetModel.predict(x_val)
print(randomForecetModel.score(x_val, y_val))
print(mean_absolute_error(y_val, pre))

"""**LinearRegression**"""

linearModel = LinearRegression(fit_intercept=False).fit(x_train, y_train)
pre = linearModel.predict(x_val)
print(linearModel.score(x_val, y_val))
print(mean_absolute_error(y_val, pre))

"""**XGBRegressor**"""

XGBoostModel = XGBRegressor().fit(x_train, y_train)
pre = XGBoostModel.predict(x_val)
print(XGBoostModel.score(x_val, y_val))
print(mean_absolute_error(y_val, pre))

"""**Lasso Regression**"""

model_lasso = Lasso(alpha=0.05).fit(x_train, y_train)
pre = model_lasso.predict(x_val)
print(model_lasso.score(x_val, y_val))
print(mean_absolute_error(y_val, pre))

"""**Ridge Regression**"""

model_ridge = Ridge().fit(x_train, y_train)
pre = model_ridge.predict(x_val)
print(model_ridge.score(x_val, y_val))
print(mean_absolute_error(y_val, pre))

"""**Summary**

according to the preveous models:
> RandomForestRegressor has the best accurecy with Mean_Absolute_Error=680
.so i will predict the Item_Outlet_Sales of the test data
"""

predected_values = randomForecetModel.predict(df_test)
# predected_values

"""now variable "predected_values" contains the predicted values"""